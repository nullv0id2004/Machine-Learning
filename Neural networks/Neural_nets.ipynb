{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9207672c-4fa7-4753-ad76-1443da0082b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c80e1c3-2f7d-49c8-8013-8ba9dbc3c39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lisa_simpson',\n",
       " 'bart_simpson',\n",
       " 'homer_simpson',\n",
       " 'charles_montgomery_burns',\n",
       " 'milhouse_van_houten',\n",
       " 'marge_simpson',\n",
       " 'moe_szyslak',\n",
       " 'krusty_the_clown',\n",
       " 'principal_skinner',\n",
       " 'ned_flanders']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = os.listdir('./../../Downloads/train')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c8f0ad1-de6e-48eb-bbbe-a81843058b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lisa_simpson': 0,\n",
       " 'bart_simpson': 1,\n",
       " 'homer_simpson': 2,\n",
       " 'charles_montgomery_burns': 3,\n",
       " 'milhouse_van_houten': 4,\n",
       " 'marge_simpson': 5,\n",
       " 'moe_szyslak': 6,\n",
       " 'krusty_the_clown': 7,\n",
       " 'principal_skinner': 8,\n",
       " 'ned_flanders': 9}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map = {'lisa_simpson' : 0,'bart_simpson' : 1,'homer_simpson': 2,'charles_montgomery_burns':3,'milhouse_van_houten':4,'marge_simpson':5,'moe_szyslak':6,'krusty_the_clown':7,'principal_skinner':8,'ned_flanders':9}\n",
    "map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fa137a-7d5f-4425-a2c7-155353aa84c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 28,  27,  25,  26,  26,  27,  29,  24,  29,  23,  29,  24,  21,\n",
       "         27,  26,  26,  27,  24,  30,  29,  26,  27,  27,  22,   1,   0,\n",
       "          0,   0],\n",
       "       [ 25,  25,  24,  24,  22,  22,  25,  24,  21,  23,  23,  23,  26,\n",
       "         24,  26,  21,  24,  29,  24,  23,  25,  29,  23,  20,   1,   0,\n",
       "          0,   0],\n",
       "       [ 28,  26,  24,  27,  28,  27,  29,  27,  32,  27,  23,  26,  21,\n",
       "         32,  27,  18,  26,  27,  23,  24,  29,  23,  23,  24,   1,   0,\n",
       "          0,   0],\n",
       "       [ 26,  24,  23,  24,  22,  20,  25,  27,  21,  23,  24,  22,  24,\n",
       "         26,  31,  24,  19,  22,  23,  24,  22,  24,  24,  15,   1,   0,\n",
       "          0,   0],\n",
       "       [ 50,  36,  23,  23,  26,  26,  25,  19,  48,  29,  29,  18, 150,\n",
       "        161,  81,  18,  29,  27,  27,  24,  23,  25,  20,  22,   1,   0,\n",
       "          0,   0],\n",
       "       [179, 177, 175, 176, 175, 175, 181, 182, 161, 162, 159, 170, 170,\n",
       "        165, 159,  91, 103, 118, 113,  26,  28,  30,  28,  18,   1,   0,\n",
       "          0,   0],\n",
       "       [177, 178, 178, 180, 179, 177, 179, 177, 179, 151, 163, 161, 163,\n",
       "        163, 162, 164, 161, 169, 165,  26,  31,  23,  28,  20,   1,   0,\n",
       "          0,   0],\n",
       "       [179, 179, 177, 177, 177, 178, 180, 176, 162, 169, 167, 162, 163,\n",
       "        166, 162, 162, 165, 160, 158,  25,  26,  28,  26,  21,   1,   0,\n",
       "          0,   0],\n",
       "       [177, 179, 175, 178, 175, 177, 181, 176, 130, 156, 166, 159, 165,\n",
       "        161, 164, 160, 165, 163, 156, 162, 133,  24,  25,  20,   1,   0,\n",
       "          0,   0],\n",
       "       [146, 137, 175, 175, 179, 181, 178, 177,  98, 164, 161, 171, 161,\n",
       "        165, 161, 166, 163, 161, 161, 165, 161,  33,  29,  23,   1,   0,\n",
       "          0,   0],\n",
       "       [175, 182, 168, 178, 178, 173, 179, 175,  88, 144, 138, 111, 151,\n",
       "        143, 158, 162, 164, 164, 166, 163,  89,  23,  31,  19,   1,   0,\n",
       "          0,   0],\n",
       "       [173, 167, 178, 180, 185, 180, 178, 179, 115, 209, 207, 207, 160,\n",
       "        211, 216, 148, 156, 164, 165, 164,  87,  28,  24,  24,   1,   0,\n",
       "          0,   0],\n",
       "       [135,  69,  50,  56,  59, 177, 178, 176, 123, 168, 186, 211, 206,\n",
       "        216, 213, 167, 164, 161, 160, 159, 157,  68,  28,  17,   1,   0,\n",
       "          0,   0],\n",
       "       [ 77,  74,  80,  50,  76, 135, 175, 175, 123, 161, 161, 160, 157,\n",
       "        157, 165, 168, 167, 164, 162, 173, 110,  26,  19,  25,   1,   0,\n",
       "          0,   0],\n",
       "       [178, 173, 177, 175, 176, 180, 176, 168, 150, 167, 159, 165, 163,\n",
       "        161, 164, 154, 139, 162, 166,  51,  26,  31,  26,  23,   1,   0,\n",
       "          0,   0],\n",
       "       [170, 175, 177, 176, 178, 180, 147, 170, 149, 121, 157, 142, 132,\n",
       "        168, 159, 157, 143, 167, 169,  96,  26,  28,  27,  18,   1,   0,\n",
       "          0,   0],\n",
       "       [169, 109, 178, 174, 183, 172, 151, 162, 164, 165, 132, 165, 164,\n",
       "        161, 151, 161, 152, 105, 100,  60,  26,  27,  27,  21,   1,   0,\n",
       "          0,   0],\n",
       "       [179, 168, 170, 191, 174, 166, 164, 163, 151, 149, 153, 160, 162,\n",
       "        158,  94, 156,  64,  26,  28,  27,  26,  26,  26,  21,   1,   0,\n",
       "          0,   0],\n",
       "       [177, 181, 174, 176, 184, 157, 164, 160, 161, 159, 138, 171, 139,\n",
       "        204, 178, 154,  28,  25,  28,  24,  26,  27,  30,  19,   1,   0,\n",
       "          0,   0],\n",
       "       [178, 177, 181, 180, 177, 156, 161, 164, 162, 163, 163, 161, 163,\n",
       "        163, 164,  30,  25,  27,  27,  25,  28,  25,  28,  22,   2,   0,\n",
       "          0,   0],\n",
       "       [180, 175, 176, 179, 176, 172, 136, 142, 162, 160, 166, 164, 165,\n",
       "        161, 165,  23,  29,  24,  25,  28,  28,  24,  24,  21,   2,   1,\n",
       "          0,   0],\n",
       "       [177, 181, 179, 177, 179, 183, 180,  91, 130, 146, 159, 160, 153,\n",
       "        135,  96,  29,  23,  26,  30,  25,  22,  27,  27,  22,   2,   1,\n",
       "          0,   0],\n",
       "       [180, 175, 176, 178, 179, 178, 177, 102, 111, 110, 111, 112, 107,\n",
       "        105, 114,  24,  27,  27,  23,  23,  25,  25,  23,  21,   2,   1,\n",
       "          0,   0],\n",
       "       [178, 181, 178, 176, 178, 183, 157, 108, 115, 110, 107, 105, 112,\n",
       "        107, 104,  26,  18,  27,  20,  22,  26,  21,  21,  21,   2,   1,\n",
       "          0,   0],\n",
       "       [127, 137, 152, 172, 185, 186, 120, 112, 110, 107, 108, 108, 109,\n",
       "        112, 109,  23,  26,  24,  26,  25,  26,  25,  25,  20,   1,   0,\n",
       "          0,   0],\n",
       "       [ 17,  31,  86,  86,  95,  81,  77, 114, 112, 109, 109, 109, 107,\n",
       "        108, 108,  44,  28,  26,  28,  26,  27,  26,  26,  22,   1,   0,\n",
       "          0,   0],\n",
       "       [ 26,  25, 105, 111, 108, 111, 107, 106, 110, 108, 109, 112, 107,\n",
       "        107, 112,  79,  24,  22,  23,  21,  22,  21,  22,  18,   1,   0,\n",
       "          0,   0],\n",
       "       [ 28,  86, 116, 110, 111, 110, 111, 112, 108, 108, 109, 113, 109,\n",
       "        110, 115, 104,  29,  26,  27,  24,  26,  25,  26,  22,   1,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "image_data = cv2.imread('./../../Downloads/train/lisa_simpson/pic_0005.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c35880be-6e69-4770-adfc-aa87c38829ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image_data, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7c010b-f65c-4e20-9ff8-8b73ba409cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset,path):\n",
    "    # Scan all the directories and create a list of labels\n",
    "    labels=os.listdir(os.path.join(path, dataset))\n",
    "    # Create lists for samples and labels\n",
    "    X=[]\n",
    "    y=[]\n",
    "    # For each label folder\n",
    "    for label in labels:\n",
    "        # And for each image in given folder\n",
    "        for file in os.listdir(os.path.join(path, dataset, label)):\n",
    "            \n",
    "            # Read the image\n",
    "            image= cv2.imread(os.path.join(path, dataset, label, file), cv2.IMREAD_UNCHANGED)\n",
    "            # And append it and a label to the lists\n",
    "            X.append(image)\n",
    "            y.append(map[label])\n",
    "# Convert the data to proper numpy arrays and return\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0008aa6-c7a6-4e82-9761-43c6e8aa111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(path):\n",
    "    X,y = load_dataset('train',path)\n",
    "    X_test, y_test = load_dataset('test',path)\n",
    "    return X,y,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce65f55-f479-46be-9757-0504de821fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,X_test,y_test = create_data('./../../Downloads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9cb1659-acc7-4393-9c56-f282d094c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = np.array(range(X.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "X=X[keys]\n",
    "y=y[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c66b5bed-1f2a-44ba-9595-fc9aad9f65b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255, 0, 255)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min(),X.max(),X_test.min(),X_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1297b37e-6ba5-431a-9db2-8b0d586d09d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e0b49ca-0bf3-4aea-880f-5a32b0b9c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float32) /255\n",
    "X_test = X_test.astype(np.float32) /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e62c8147-1edc-4673-988d-292f5bae5871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, (8000, 28, 28))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min(),X.max(),X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "722087d4-5726-4f13-add8-0af9cb719a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 784), (2000, 784))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.reshape(X.shape[0],-1)\n",
    "X_test = X_test.reshape(X_test.shape[0],-1)\n",
    "X.shape , X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445abf2-bb80-4dac-a241-4a3d75d407f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8206a1f-37f7-4612-8060-2f5b89279802",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons, weight_regularizer_l1=0, weight_regularizer_l2=0,bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        self.weights = 0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T , dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0 , keepdims=True)\n",
    "        if self.weight_regularizer_l1>0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights<0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "        if self.weight_regularizer_l2>0:\n",
    "            self.dweights +=2* self.weight_regularizer_l2 * \\self.weights\n",
    "        if self.bias.regularizer_l1>0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases<0] = -1\n",
    "            self.dbiases \u0000= self.bias_regularizer_l1* dL1\n",
    "        if self.bias_regularizer_l2>0:\n",
    "            self.dbiases += 2* self.bias_regularizer_l2 * \u0000b\u0000i\u0000f.biases\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.weights, self.biases\n",
    "    def set_parameters(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "\n",
    "class Layer_Dropout:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        if not training:\n",
    "            self.output = inputs.copy()\n",
    "            return\n",
    "        self.binary_mask = np.random.binomial(1,self.rate, size=inputs.shape) / self.rate\n",
    "        self.output = inputs * self.binary_mask\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask\n",
    "    \n",
    "        \n",
    "\n",
    "class Layer_Input:\n",
    "    def forward(self,inputs,training):\n",
    "        self.output = inputs\n",
    "    \n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs<=0] = 0\n",
    "    def predictions(self,outputs):\n",
    "        return outputs\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs, training):\n",
    "        self.inputs = inputs\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims = True))\n",
    "        probabilites = exp_values / np.sum(exp_values, axis=1, keepdims = True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            jacobian_matrix = np.diagflat(single_output) - (np.dot(single_output, single_output.T))\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "    def predicitons(self, outputs):\n",
    "        return np.argmax(outputs, axis=1)\n",
    "\n",
    "class Loss:\n",
    "    def regularization_loss(self):\n",
    "        regularization_loss=0\n",
    "        for layer in self.trainable_layers:\n",
    "            \n",
    "            # if layer.weight_regularizer_l1 > 0:\n",
    "            #     regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs((layer.weights))\n",
    "                                                                            \n",
    "            # if layer.weight_regularizer_l2 > 0:\n",
    "            #     regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "                \n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "                \n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "                \n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def remember_trainable_layers(self, trainable_layers):\n",
    "        self.trainable_layers = trainable_layers\n",
    "\n",
    "    def calculate(self, output, y, *, include_regularization=False):\n",
    "        sample_losses = self.forward(output,y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "\n",
    "    def calculate_accumulated(self,*,include_regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "        if not include_regulariztion:\n",
    "            return data_loss\n",
    "        return data_loss, self.regularization_loss()\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "        \n",
    "\n",
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, dacay=0,epsilon=1e-7,beta_1=0.9,beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. +self.decay * self.iterations))\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weights_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1- self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1-self.beta_1)* layer.dbiases\n",
    "        weight_momentums_corrected = layer.weight_momentums/(1-self.beta_1 ** (self.iterations +1))\n",
    "        bias_momentums_corrected = layer.bias_momentums/(1-self.beta_1 ** (self.iterations +1))\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1-self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1-self.beta_2) * layer.dbiases**2\n",
    "        weight_cache_corrected = layer.weight_cache / (1-self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1-self.beta_2 ** (self.iterations +1))\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected/(np.sqrt(weight_cache_corrected)+self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected/(np.sqrt(bias_cache_corrected)+self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations +=1\n",
    "\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class Accuracy:\n",
    "    def calculate(self,predictions,y):\n",
    "        comparisions =  self.compare(predictions,y)\n",
    "        accuracy = np.mean(comparisions)\n",
    "        self.accumulated_sum += np.sum(comparisions)\n",
    "        self.accumulated_count += len(comparisions)\n",
    "        return accuracy\n",
    "    def calculate_accumulated(self):\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        return accuracy\n",
    "    def new_pass(self):\n",
    "        self.accumualted_sum = 0\n",
    "        self.accumulated_count = 0\n",
    "\n",
    "class Accuracy_Categorical(Accuracy):\n",
    "    def init(self,y):\n",
    "        pass\n",
    "    def compare(self,predicitions,y):\n",
    "        if len(y.shape) == 2:\n",
    "            y = np.argmax(y, axis=1)\n",
    "        return predictions == y\n",
    "\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    def backward(self,dvalues,y_true):\n",
    "        samples = len(dvalues)\n",
    "        if len(y_true.shape)==2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[range(samples),y_true] -= 1\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.softmax_classifier_output = None\n",
    "    def add(self, Layer):\n",
    "        self.layers.append(layer)\n",
    "    def set(self, * , loss, optimizer, accuracy):\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.accuracy =  accuracy\n",
    "    def finalize(self):\n",
    "        self.input_layer = Layer_Input()\n",
    "        layer_count = len(self.layers)\n",
    "        self.trainable_layers = []\n",
    "        for i in range(layer_count):\n",
    "            if i==0:\n",
    "                self.layers[i].prev = self.input_layer\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            elif i< layer_count - 1:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.layers[i+1]\n",
    "            else:\n",
    "                self.layers[i].prev = self.layers[i-1]\n",
    "                self.layers[i].next = self.loss\n",
    "                self.output_layer_activation = self.layers[i]\n",
    "            if hasattr(self.layers[i] , 'weight'):\n",
    "                self.trainable_layers.append(self.layers[i])\n",
    "            self.loss.remember_trainable_layers(self.trainable_layers)\n",
    "        if isinstance(self.layers[-1], Activation_Softmax) and isinstance(self.loss, Loss_CategoricalCrossentropy):\n",
    "            self.softmax_classifier_output = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "    def train(self,X,y,*,epochs=1, batch_size=None,print_every=1,validation_data=None):\n",
    "        self.accuracy.init(y)\n",
    "        train_steps = 1\n",
    "        if batch_size is not None:\n",
    "            train_steps = len(X) // batch_size\n",
    "            if train_steps * batch_size < len(X):\n",
    "                train_steps +=1\n",
    "        for epoch in range(1,epoch+1):\n",
    "            print(f'epoch:{epoch}')\n",
    "            self.loss.new_pass()\n",
    "            self.accuracy.new_pass()\n",
    "            for step in range(train_steps):\n",
    "                if batch_size is None:\n",
    "                    batch_X=X\n",
    "                    batch_y =y\n",
    "                else:\n",
    "                    batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "                    batch_y = y[step*batch_size:(step+1)*batch_size]\n",
    "                output = self.forward(batch_X, training=True)\n",
    "                data_loss, regularization_loss = self.loss.calculate(output,batch_y, include_regularization = True)\n",
    "                loss = data_loss + regularization_loss\n",
    "                predictions = self.output_layer_activation.prediction(output)\n",
    "                accuracy = self.accuracy.calculate(predictions, batch_y)\n",
    "                self.backward(output, batch_y)\n",
    "                self.optimizer.pre_update_params()\n",
    "                for layer in self.trainable_layers:\n",
    "                    self.optimizer.update_params(layer)\n",
    "                self.optimizer.post_update_params()\n",
    "                if not step % print_every or step == train_steps - 1:\n",
    "                    print(f'step: {step},' + f'acc: {accuracy:.3f},' + f'loss: {loss:.3f},(' + f'data_loss: {data_loss: .3f},' + f'reg_loss: {regularization_loss:.3f}),' + f'lr: {self.optimizer.current_learning_rate}')\n",
    "            epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularizaiton=True)\n",
    "            epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
    "            epoch_accuracy = self.accuracy.calculate_accumulated()\n",
    "            print(f'training,' + f'acc: {epoch_accuracy:.3f},' + f'loss: {epoch_loss:.3f},(' + f'data_loss: {epoch_data_loss: .3f},' + f'reg_loss: {epoch_regularization_loss:.3f}),' + f'lr: {self.optimizer.current_learning_rate}')\n",
    "            if validation_data is not None:\n",
    "                self.evaluate(*validation_data, batch_size = batch_size)\n",
    "            \n",
    "    def evaluate(self, X_val,y_val,*,batch_size=None):\n",
    "        validation_steps = 1\n",
    "        if batch_size is not None:\n",
    "            validation_steps = len(X_val) // batch_size\n",
    "            if validation_steps * batch_size < len(X_val):\n",
    "                validation_steps += 1\n",
    "        self.loss.new_pass()\n",
    "        self.accuracy.new_pass()\n",
    "        for step in range(validation_steps):\n",
    "            if batch_size is None:\n",
    "                batch_X = X_val\n",
    "                batch_y = y_val\n",
    "            else:\n",
    "                batch_X = X_val[step*batch_size:(step-1)*batch_size]\n",
    "                batch_y = y_val[step*batch_size:(step-1)*batch_size]\n",
    "            output = self.forward(batch_X, training=False)\n",
    "            self.loss.calculate(output,batch_y)\n",
    "            predictions = self.output_layer_activation.predicitons(output)\n",
    "            self.accuracy.calculate(predictions, batch_y)\n",
    "        validation_loss = self.loss.calculate_accumulated()\n",
    "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
    "        print(f'validation,' + f'acc: {validation_accuracy: .3f},' + f'loss:{validation_loss:3f}')\n",
    "    def predict(self,X,*,batch_size=None):\n",
    "        prediction_steps = 1\n",
    "        if batch_size is not None:\n",
    "            prediction_steps = len(X) // batch_size\n",
    "            if prediciton_steps * batch_size < len(X):\n",
    "                prediction_steps += 1\n",
    "        output = []\n",
    "        for step in range(prediction_steps):\n",
    "            if batch_size is None:\n",
    "                batch_X = X\n",
    "            else:\n",
    "                batch_X = X[step*batch_size:(step+1)*batch_size]\n",
    "            batch_output = self.forward(batch_X, training=False)\n",
    "            output.append(batch_output)\n",
    "        return np.vstack(output)\n",
    "    def forward(self,X,training):\n",
    "        self.input_layer.forward(X,training)\n",
    "        for layer in self.layers:\n",
    "            layer.forward(layer.prev.output, training)\n",
    "        return layer.output\n",
    "    def backward(self,output,y):\n",
    "        if self.softmax_classifier_output is not None:\n",
    "            self.softmax_classifier_output.backward(output,y)\n",
    "            self.layers[-1],dinputs = self.softmax_classifier_output.dinputs\n",
    "            for layer in reversed(self.layers[:-1]):\n",
    "                layer.backward(layer.next.dinputs)\n",
    "            return\n",
    "\n",
    "        self.loss.backward(output,y)\n",
    "        for layer in reversed(self.layers):\n",
    "            layer.backward(layer.next.dinputs)\n",
    "\n",
    "model = Model()\n",
    "model.add(Layer_Dense(X.shape[1],128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128,128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128,10))\n",
    "model.add(Activation_Softmax())\n",
    "model.set(Loss = Loss_CategoricalCrossentropy(),optimizer=Optimizer_Adam(decay=1e-4),accuracy=Accuracy_Categorical())\n",
    "model.finalize()\n",
    "model.train(X,y,validation_data=(X_test,y_test),epochs=10,batch_size=128,print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee705df4-7002-43ed-b4ba-918325889ae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m()\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Layer_Dense(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Activation_ReLU())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.add(Layer_Dense(X.shape[1],128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128,128))\n",
    "model.add(Activation_ReLU())\n",
    "model.add(Layer_Dense(128,10))\n",
    "model.add(Activation_Softmax())\n",
    "model.set(Loss = Loss_CategoricalCrossentropy(),optimizer=Optimizer_Adam(decay=1e-4),accuracy=Accuracy_Categorical())\n",
    "model.finalize()\n",
    "model.train(X,y,validation_data=(X_test,y_test),epochs=10,batch_size=128,print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573a6002-8a7b-44b5-9984-cc654a587ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
